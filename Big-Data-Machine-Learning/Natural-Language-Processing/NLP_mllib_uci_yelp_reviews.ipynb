{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & Binary Classification: Yelp Reviews\n",
    "https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#\n",
    "\n",
    "** Dataset Information: **\n",
    "\n",
    "1000 sentences labelled with positive or negative sentiment from yelp.com \n",
    "\n",
    "** Attribute Information: (1 features and 1 class)**\n",
    "\n",
    "- Sentences\t\n",
    "- Score : 1 (for positive) or 0 (for negative)\t\n",
    "\n",
    "** Objective of this project **\n",
    "\n",
    "predict sentiment (positive or negative) from sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/danny/spark-2.2.1-bin-hadoop2.7')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('yelp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "df = spark.read.csv('yelp_labelled.txt',\n",
    "                    inferSchema=True,sep='\\t')\n",
    "# Inspect Data\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('_c0','text').withColumnRenamed('_c1','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Wow... Loved this...|    1|\n",
      "|  Crust is not good.|    0|\n",
      "|Not tasty and the...|    0|\n",
      "|Stopped by during...|    1|\n",
      "|The selection on ...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text='Wow... Loved this place.', label=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|                text|             label|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|                1000|              1000|\n",
      "|   mean|                null|               0.5|\n",
      "| stddev|                null|0.5002501876563867|\n",
      "|    min|!....THE OWNERS R...|                 0|\n",
      "|    max|you can watch the...|                 1|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  500|\n",
      "|    0|  500|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import (Tokenizer,StopWordsRemover,\n",
    "                                CountVectorizer,IDF,StringIndexer,\n",
    "                                VectorAssembler,OneHotEncoder)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pipeline for data prep **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                text|label|length|\n",
      "+--------------------+-----+------+\n",
      "|Wow... Loved this...|    1|    24|\n",
      "|  Crust is not good.|    0|    18|\n",
      "|Not tasty and the...|    0|    41|\n",
      "|Stopped by during...|    1|    87|\n",
      "|The selection on ...|    1|    59|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+----------+-----------+\n",
      "|label|avg(label)|avg(length)|\n",
      "+-----+----------+-----------+\n",
      "|    1|       1.0|     55.882|\n",
      "|    0|       0.0|      60.75|\n",
      "+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new length feature (feature engineering)\n",
    "df= df.withColumn('length',length(df['text']))\n",
    "df.show(5)\n",
    "df.groupby('label').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag-of-words model (feature transformations)\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine features into a single column\n",
    "assembler = VectorAssembler(inputCols=['tf_idf','length'],\n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(2584,[18,64,2222...|\n",
      "|    0|(2584,[14,759,258...|\n",
      "|    0|(2584,[145,351,19...|\n",
      "|    1|(2584,[34,64,186,...|\n",
      "|    1|(2584,[3,72,137,3...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "data_prep_pipe = Pipeline(stages=[tokenizer,stopremove,count_vec,idf,assembler])\n",
    "clean_data = data_prep_pipe.fit(df).transform(df)\n",
    "final_data = clean_data.select(['label','features'])\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split Train Test sets **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|               690|\n",
      "|   mean|0.4681159420289855|\n",
      "| stddev|0.4993443462061108|\n",
      "|    min|                 0|\n",
      "|    max|                 1|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|               310|\n",
      "|   mean|0.5709677419354838|\n",
      "| stddev|0.4957381788788524|\n",
      "|    min|                 0|\n",
      "|    max|                 1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 101 #for reproducibility\n",
    "train_data,test_data = final_data.randomSplit([0.7,0.3],seed=seed)\n",
    "train_data.describe().show()\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(2584,[0,2,25,26,...|    0|       0.0|\n",
      "|(2584,[0,4,25,42,...|    0|       1.0|\n",
      "|(2584,[0,4,62,68,...|    0|       0.0|\n",
      "|(2584,[0,5,19,33,...|    0|       0.0|\n",
      "|(2584,[0,14,2583]...|    0|       1.0|\n",
      "|(2584,[0,17,143,2...|    0|       0.0|\n",
      "|(2584,[0,17,621,1...|    0|       0.0|\n",
      "|(2584,[0,24,153,1...|    0|       1.0|\n",
      "|(2584,[0,49,145,1...|    0|       1.0|\n",
      "|(2584,[0,62,139,1...|    0|       0.0|\n",
      "|(2584,[0,86,225,2...|    0|       1.0|\n",
      "|(2584,[1,5,31,126...|    0|       0.0|\n",
      "|(2584,[1,5,127,33...|    0|       0.0|\n",
      "|(2584,[1,6,7,10,1...|    0|       0.0|\n",
      "|(2584,[1,8,266,57...|    0|       0.0|\n",
      "|(2584,[1,9,997,19...|    0|       1.0|\n",
      "|(2584,[1,15,22,43...|    0|       0.0|\n",
      "|(2584,[1,26,36,22...|    0|       0.0|\n",
      "|(2584,[1,36,39,98...|    0|       1.0|\n",
      "|(2584,[1,69,78,67...|    0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy of model at predicting spam: 71.8%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Train the model\n",
    "nb = NaiveBayes()\n",
    "spam_predictor = nb.fit(train_data)\n",
    "# Make predictions\n",
    "predictions = spam_predictor.transform(test_data)\n",
    "predictions.select(['features','label','prediction']).show()\n",
    "# Evaluate the models\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(predictions)\n",
    "print(\"Accuracy of model at predicting spam: {:.1f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
